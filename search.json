[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Imposters in Data: Anomaly Detection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s Classify\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering: Making Sense of Seemingly Random Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Particle Filters for Real-Time State Estimation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression: Unraveling the Intricacies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Particle_Filter/Particle_Filter.html#introduction",
    "href": "posts/Particle_Filter/Particle_Filter.html#introduction",
    "title": "Using Particle Filters for Real-Time State Estimation",
    "section": "Introduction",
    "text": "Introduction\nWelcome to our deep dive into the world of particle filters, a fascinating and powerful tool in probability theory. Particle filters, part of the Sequential Monte Carlo method family, are essential in various applications like robotics, signal processing, and finance. They offer a dynamic approach to estimating systems over time, especially in non-linear and non-Gaussian contexts."
  },
  {
    "objectID": "posts/Particle_Filter/Particle_Filter.html#the-basics",
    "href": "posts/Particle_Filter/Particle_Filter.html#the-basics",
    "title": "Using Particle Filters for Real-Time State Estimation",
    "section": "The Basics",
    "text": "The Basics\n\nBayesian Filtering\nAt the heart of particle filters lies Bayesian filtering. This approach updates the probability estimate for a system’s state as new information becomes available. Unlike traditional methods, Bayesian filters continuously refine predictions, making them ideal for dealing with uncertainty in dynamic systems.\n\n\nSequential Monte Carlo Methods\nParticle filters are a subset of Sequential Monte Carlo (SMC) methods. SMC represents a probability distribution by a set of samples (or particles) and sequentially updates these samples using the Bayesian framework. It’s a versatile tool, particularly effective in scenarios where other methods falter due to model complexities.\n\n\nParticles\nIn particle filters, ‘particles’ represent possible states of the system, each with a weight indicating its probability. Think of them as tiny probes exploring different scenarios of how a system could evolve."
  },
  {
    "objectID": "posts/Particle_Filter/Particle_Filter.html#the-mechanics",
    "href": "posts/Particle_Filter/Particle_Filter.html#the-mechanics",
    "title": "Using Particle Filters for Real-Time State Estimation",
    "section": "The Mechanics",
    "text": "The Mechanics\nIn this section, we will explore the entire process of designing a particle filter.\n\nProblem Definiton\nWe will be considering the system from my previous blog post for this post. All the required libraries are loaded. Though we do not need all of them in this partcular context, I like to keep them on hand just in case.\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport threading\nimport tensorflow as tf\nimport time\nfrom keras.layers import Input, Dense, Dropout, Lambda, concatenate, Conv2D, MaxPooling2D, Flatten, LSTM, BatchNormalization, MultiHeadAttention, Conv1D, GlobalAveragePooling1D, LayerNormalization, GlobalMaxPooling1D\nfrom keras.models import Model, load_model\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2\nfrom keras import backend as K\nfrom scipy.sparse import coo_matrix\nimport math\nfrom scipy.fftpack import fft, ifft\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom scipy.linalg import eigvalsh, eigh\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n\n\nNeural Network Model\nThe neural network model is loaded and this model acts as a state-trasition function, which has an input of 21 current states of the system and and will output 21 states of the next time-step. The model validation is shown below with the test plot.\n\nmodel = load_model('Model3_cnn_lstm.keras',compile=False)\n\nfilename = \"21_points.csv\"\n\n## Reading Data\ndata = pd.read_csv(filename, sep = \",\")\ndata = data.to_numpy()\n\n## Preparing Data\ndata_train = data[:round(data.shape[0]*0.8),:]\ndata_test = data[round(data.shape[0]*0.8):,:]\n\n\n\npred_test = model.predict(data_test)\ntime_step = 150\n\nplt.plot(np.arange(data_test.shape[1]),data_test[time_step,:],np.arange(data_test.shape[1]),pred_test[time_step+1,:] )\n\n313/313 [==============================] - 10s 29ms/step\n\n\n\n\n\n\n\nIntialization\nParticles are initialized, often randomly, to represent the initial state distribution. But since I have time-series data of the system under consideration, I have initialized the filter with actual states at a random-time step. This helps the filter converge faster as the initial states is already in the desired domain. As metioned in my previous blog post, only certain number of states can be measured using sensors. These positions are defined and for the purpose of this post, I will only be using 50 particles in this filter.\n\ninit_state = data[1000,:]\nnum_particles = 50\nnum_states = 21\nsensor_std_dev = 0.5\nsensor_pos = [2, 4, 6, 8, 10, 12, 14, 16, 18, 19]\ntime_steps = 100\n\nThis function uses the Neural Network model to predict the states based on the current states. This will be used as the motion model.\n\ndef predict_particles(particles, num_particles):\n    for i in range(num_particles):\n        particles[i,:] = NN(particles[i,:])\n    \n    return particles \n\ndef NN(state):\n    reshape_state = state.reshape(1,-1)\n    predict_state  = model.predict(reshape_state, verbose = 0)\n    return predict_state.reshape(num_states)\n\nThis collects the senosr reading from available sensors and hence provide the actual states and hence serves as the measurement model.\n\ndef sensor(actual_state):\n    return actual_state[sensor_pos]\n\nHere each particle’s weight is updated. The closer a particle’s predicted state is to the actual observed data, the higher its weight.\n\ndef update_weights(particles, weights, measurement,sensor_std_dev):\n    for i in range(num_particles):\n        weight = 1.0\n        for j in range(len(measurements)):\n            sensor_measurement = measurements[j]\n            weight *= np.exp(-(particles[i, j] - sensor_measurement) ** 2 / (2 * sensor_std_dev ** 2))\n            \n            weights[i] = weight\n\n    weights += 1.e-300  \n    weights /= np.sum(weights) \n        \n\nHere, the resampling particles takes place. Resampling combats particle degeneracy, where over time, most particles tend to have negligible weight. It focuses computational resources on areas with higher probability.\n\n        \ndef resample_particles(particles, weights, num_particles):\n    cumulative_sum = np.cumsum(weights)\n    cumulative_sum[-1] = 1\n    indexes = np.searchsorted(cumulative_sum, np.random.rand(num_particles))\n    return particles[indexes, :], np.ones(num_particles) / num_particles\n\ndef estimate_state(particles):\n    return np.mean(particles, axis = 0)\n\n\ndef init_particles(num_particles,num_states):\n    return data[1000:1000+num_particles,:]\n\nparticles = init_particles(num_particles,num_states)\nweights = np.ones(num_particles)/num_particles\nstate_estimate = np.zeros((time_steps, num_states))\n\n\n\nRunning the Filter\n\nfor t in tqdm(range(time_steps), desc=\"Processing\", unit = \"Iteration\"):\n    \n    tt = time.time()  \n    actual_state = data[2000+t,:]\n    measurements = sensor(actual_state)\n    particles = predict_particles(particles,num_particles)\n    update_weights(particles, weights, measurements,sensor_std_dev)\n    particles, weights = resample_particles(particles, weights, num_particles)\n    \n    state_estimate[t,:] = estimate_state(particles)\n\nProcessing: 100%|██████████| 100/100 [15:34&lt;00:00,  9.34s/Iteration]\n\n\n\n\nPerformance Evaluation\n\nstep = 95\nplt.plot(np.arange(data_test.shape[1]),data_test[1000+step,:],np.arange(data_test.shape[1]),state_estimate[step,:] )\nplt.legend([\"Actual\",\"Filter\"])\n\n&lt;matplotlib.legend.Legend at 0x23b98e968d0&gt;\n\n\n\n\n\nHere we can see that the filter has not actually converged to the actual states of the sysytem but at the same time, the results look promising. With better tuning of the parameters and more number of particles, we will be able to achieve better performance."
  },
  {
    "objectID": "posts/Particle_Filter/Particle_Filter.html#the-challenges",
    "href": "posts/Particle_Filter/Particle_Filter.html#the-challenges",
    "title": "Using Particle Filters for Real-Time State Estimation",
    "section": "The Challenges",
    "text": "The Challenges\n\nParticle Degeneracy\nA major challenge with particle filters is degeneracy, where after several iterations, only a few particles have significant weights. Resampling is a common solution, though it needs to be done carefully to avoid sample impoverishment.\n\n\nSample Impoverishment\nThis occurs when diversity among particles decreases, often due to excessive resampling. Techniques like adding random noise to the particles during resampling can help maintain diversity.\n\n\nComputational Complexity\nParticle filters can be computationally intensive. Optimization techniques, such as using efficient data structures or parallel processing, are crucial for practical applications."
  },
  {
    "objectID": "posts/Particle_Filter/Particle_Filter.html#conclusion",
    "href": "posts/Particle_Filter/Particle_Filter.html#conclusion",
    "title": "Using Particle Filters for Real-Time State Estimation",
    "section": "Conclusion",
    "text": "Conclusion\nParticle filters are a robust, flexible tool for dynamic system estimation in uncertain environments. As technology evolves, so too will the applications and efficiency of particle filters, making them an exciting area of ongoing research and development."
  },
  {
    "objectID": "posts/Classification/Classification.html#introduction",
    "href": "posts/Classification/Classification.html#introduction",
    "title": "Let’s Classify",
    "section": "Introduction",
    "text": "Introduction\nClassification is a cornerstone of machine learning, where the objective is to categorize data into predefined classes. It’s a critical tool for pattern recognition, often used in applications such as email filtering, language detection, and medical diagnosis."
  },
  {
    "objectID": "posts/Classification/Classification.html#the-definition",
    "href": "posts/Classification/Classification.html#the-definition",
    "title": "Let’s Classify",
    "section": "The Definition",
    "text": "The Definition\nIn machine learning, classification refers to the process of predicting the category of a given data point. It’s a form of supervised learning where the model is trained on a labeled dataset."
  },
  {
    "objectID": "posts/Classification/Classification.html#real-world-applications",
    "href": "posts/Classification/Classification.html#real-world-applications",
    "title": "Let’s Classify",
    "section": "Real-World Applications",
    "text": "Real-World Applications\nEmail Filtering: Classifying emails into spam and non-spam. Medical Diagnosis: Identifying diseases based on patient records. Financial Analysis: Detecting fraudulent transactions. ## The Types Classification tasks are generally divided into binary, multi-class, and multi-label classifications.\n\nBinary Classification\nThe simplest form of classification where there are only two classes. For example, an email is either spam (positive class) or not spam (negative class).\n\n\nMulti-Class Classification\nInvolves categorizing data into more than two classes. For example, sorting animals into categories like mammals, birds, and reptiles.\n\n\nMulti-Label Classification\nA scenario where each data point can belong to multiple classes. For example, a movie can be both a comedy and a drama."
  },
  {
    "objectID": "posts/Classification/Classification.html#the-algorithms",
    "href": "posts/Classification/Classification.html#the-algorithms",
    "title": "Let’s Classify",
    "section": "The Algorithms",
    "text": "The Algorithms\nSeveral algorithms are commonly used for classification tasks, each with its strengths and weaknesses.\n\nDecision Trees\nDecision Trees classify instances by sorting them down the tree from the root to some leaf node, which provides the classification.\n\n\nSupport Vector Machines (SVM)\nSVMs are effective in high-dimensional spaces and are versatile as they can be used for both classification and regression tasks.\n\n\nNaive Bayes\nBased on Bayes’ theorem, Naive Bayes classifiers work well in many real-world situations, such as document classification and spam filtering.\n\n\nNeural Networks\nNeural Networks are particularly well-suited for complex classification problems, such as image and speech recognition."
  },
  {
    "objectID": "posts/Classification/Classification.html#python-implementation-classifying-the-iris-dataset",
    "href": "posts/Classification/Classification.html#python-implementation-classifying-the-iris-dataset",
    "title": "Let’s Classify",
    "section": "Python Implementation: Classifying the Iris Dataset",
    "text": "Python Implementation: Classifying the Iris Dataset\nThe Iris dataset contains 150 instances of iris plants, classified into three species based on the size of their sepals and petals.\n\n# Import necessary libraries\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data[:, :2]  # We take only the first two features for simplicity\ny = iris.target\n\n# Visualize the data\nplt.figure(figsize=(8, 6))\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolor='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\nplt.title('Iris Dataset - Classification')\nplt.show()"
  },
  {
    "objectID": "posts/Classification/Classification.html#model-training-and-evaluation",
    "href": "posts/Classification/Classification.html#model-training-and-evaluation",
    "title": "Let’s Classify",
    "section": "Model Training and Evaluation",
    "text": "Model Training and Evaluation\n\nPreparing the Data\nBefore training the model, we need to split our data into a training set and a test set. This allows us to evaluate our model on unseen data.\n\nfrom sklearn.model_selection import train_test_split\n# Splitting the dataset into the Training set and Test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n\nTraining the KNN Model\nKNN works by finding the nearest data points in the training set to a given data point in the test set and then classifying the test data point into the majority class among those nearest neighbors.\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Initializing the KNN Classifier\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Fitting the model with the training data\nknn.fit(X_train, y_train)\n\nKNeighborsClassifier(n_neighbors=3)"
  },
  {
    "objectID": "posts/Classification/Classification.html#evaluating-the-model",
    "href": "posts/Classification/Classification.html#evaluating-the-model",
    "title": "Let’s Classify",
    "section": "Evaluating the Model",
    "text": "Evaluating the Model\nAfter training our model, we evaluate its performance using the test set. Common evaluation metrics for classification models include accuracy, precision, recall, and the confusion matrix.\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Predicting the Test set results\ny_pred = knn.predict(X_test)\n\n# Confusion Matrix\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Classification Report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Accuracy Score\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\nplt.figure(figsize=(8, 6))\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap=plt.cm.Set1, edgecolor='k')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('KNN Classifier - Test Set')\nplt.show()\n\nConfusion Matrix:\n[[19  0  0]\n [ 0  7  6]\n [ 0  5  8]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        19\n           1       0.58      0.54      0.56        13\n           2       0.57      0.62      0.59        13\n\n    accuracy                           0.76        45\n   macro avg       0.72      0.72      0.72        45\nweighted avg       0.76      0.76      0.76        45\n\nAccuracy: 0.7555555555555555"
  },
  {
    "objectID": "posts/Classification/Classification.html#conclusion",
    "href": "posts/Classification/Classification.html#conclusion",
    "title": "Let’s Classify",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve explored the fundamental concept of classification in machine learning, illustrated by a practical example using the Iris dataset. We’ve seen how different types of classification tasks can be approached and explored some key algorithms that are commonly used in this domain.\nThe example with the K-Nearest Neighbors algorithm on the Iris dataset demonstrated not just the process of training a machine learning model but also the crucial steps of evaluating its performance. The insights gained from this exercise are invaluable in understanding how classification models work and how they can be applied to real-world problems."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Website",
    "section": "",
    "text": "My name in Nipun Nambiar. I am a Master’s student at Virginia Tech in the Mechanical Engineering department. I like to workout and eat good food. I game a little bit and watch a lot of movies."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "posts/Anomaly/Anomaly.html#introduction",
    "href": "posts/Anomaly/Anomaly.html#introduction",
    "title": "Imposters in Data: Anomaly Detection",
    "section": "Introduction",
    "text": "Introduction\nIn the world of data science, anomaly detection stands out as a key technique for identifying unusual patterns within data sets. These “anomalies” can often signify significant, sometimes critical, information, such as fraudulent activity in banking transactions or a malfunction in a production line. In this blog post, we delve into anomaly detection, using the well-known Iris Dataset as our base for exploration. While the Iris Dataset is traditionally used for classification problems, we’ll creatively adapt it to demonstrate anomaly detection."
  },
  {
    "objectID": "posts/Anomaly/Anomaly.html#the-definition",
    "href": "posts/Anomaly/Anomaly.html#the-definition",
    "title": "Imposters in Data: Anomaly Detection",
    "section": "The Definition",
    "text": "The Definition\nBefore we dive into the technicalities, let’s understand what anomaly detection really is. In the simplest terms, anomaly detection is about identifying data points that deviate significantly from the majority of the data. It’s like finding a needle in a haystack, where the needle represents an anomaly in a vast dataset."
  },
  {
    "objectID": "posts/Anomaly/Anomaly.html#the-applications",
    "href": "posts/Anomaly/Anomaly.html#the-applications",
    "title": "Imposters in Data: Anomaly Detection",
    "section": "The Applications",
    "text": "The Applications\nFinance: Identifying fraudulent transactions. Healthcare: Spotting rare diseases or unusual patient readings. Industrial: Monitoring equipment to predict and prevent failures. Cybersecurity: Detecting intrusions or other malicious activities. The Iris Dataset: A Brief Overview The Iris Dataset is a classic in the field of machine learning and statistics. It contains 150 observations of iris flowers, including measurements of the sepals and petals. Typically used for classification, we’ll use this dataset to identify one species of iris as an “anomaly,” thereby showcasing how anomaly detection works in a simple context."
  },
  {
    "objectID": "posts/Anomaly/Anomaly.html#why-the-iris-dataset",
    "href": "posts/Anomaly/Anomaly.html#why-the-iris-dataset",
    "title": "Imposters in Data: Anomaly Detection",
    "section": "Why the Iris Dataset?",
    "text": "Why the Iris Dataset?\nWell-understood: Its simplicity and familiarity make it an excellent choice for demonstration purposes. Diverse Features: With multiple features to analyze, it provides a realistic scenario for anomaly detection. Implementing Anomaly Detection Now, let’s get to the exciting part - coding our anomaly detection model using Python.\n\nThe Environment\n\n# Import necessary libraries\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import IsolationForest\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nLoading and Preparing the Iris Dataset\n\n# Load the Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Consider 'Iris-setosa' as normal and others as anomalies\ny = (y != 0) * 1  # 0 for normal, 1 for anomaly\n\n\n\nAnomaly Detection with Isolation Forest\n\n# Apply Isolation Forest\niso_forest = IsolationForest(contamination=0.33, random_state=42)\npreds = iso_forest.fit_predict(X)\n\n# Adjust predictions for visualization\npreds = (preds == -1) * 1\n\n\n\nThe Visualization\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=preds, style=y, palette=['blue', 'red'])\nplt.title('Anomaly Detection in Iris Dataset')\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.show()"
  },
  {
    "objectID": "posts/Anomaly/Anomaly.html#the-result",
    "href": "posts/Anomaly/Anomaly.html#the-result",
    "title": "Imposters in Data: Anomaly Detection",
    "section": "The Result",
    "text": "The Result\nThe visualization clearly delineates the detected anomalies (in red) from the normal data points (in blue). The Isolation Forest algorithm effectively segregates the ‘Iris-setosa’ species, treating it as an anomaly compared to the other species."
  },
  {
    "objectID": "posts/Anomaly/Anomaly.html#conclusion",
    "href": "posts/Anomaly/Anomaly.html#conclusion",
    "title": "Imposters in Data: Anomaly Detection",
    "section": "Conclusion",
    "text": "Conclusion\nThis exercise with the Iris Dataset illustrates how anomaly detection can be applied even in simple, well-understood datasets. The techniques used here can be scaled and modified for more complex and larger datasets in various domains, from finance to healthcare. The key takeaway is the versatility and utility of anomaly detection in extracting meaningful insights from data."
  },
  {
    "objectID": "posts/Clustering/Clustering.html#introduction",
    "href": "posts/Clustering/Clustering.html#introduction",
    "title": "Clustering: Making Sense of Seemingly Random Data",
    "section": "Introduction",
    "text": "Introduction\nIn this post, we delve into the fascinating world of clustering, a cornerstone of unsupervised learning in machine learning and data science. Clustering helps us find inherent patterns in unlabeled data, grouping similar items together. It has a myriad of applications, from customer segmentation in marketing to image segmentation in computer vision."
  },
  {
    "objectID": "posts/Clustering/Clustering.html#the-fundamentals",
    "href": "posts/Clustering/Clustering.html#the-fundamentals",
    "title": "Clustering: Making Sense of Seemingly Random Data",
    "section": "The Fundamentals",
    "text": "The Fundamentals\n\nDefinition and Purpose\nClustering involves grouping a set of objects such that objects in the same group are more similar to each other than to those in other groups. It’s a method to uncover the hidden structure within data.\n\n\nTypes of Clustering\nThere are several types of clustering methods:\nPartitional Clustering: Divides the dataset into distinct groups. Hierarchical Clustering: Builds a hierarchy of clusters. Density-Based Clustering: Forms clusters based on the density of data points."
  },
  {
    "objectID": "posts/Clustering/Clustering.html#the-algorithms",
    "href": "posts/Clustering/Clustering.html#the-algorithms",
    "title": "Clustering: Making Sense of Seemingly Random Data",
    "section": "The Algorithms",
    "text": "The Algorithms\n\nK-Means Clustering\nK-Means is a popular partitional clustering algorithm. It partitions data into ‘K’ distinct clusters based on their features.\n\nPython Example: K-Means Clustering\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\n# Setting environment variable to avoid memory leak warning\nos.environ[\"OMP_NUM_THREADS\"] = \"2\"\n\n# Generating synthetic data\nX, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n\n# Applying K-Means with explicit n_init\nkmeans = KMeans(n_clusters=4, n_init=10)\nkmeans.fit(X)\ny_kmeans = kmeans.predict(X)\n\n# Plotting the clusters\nplt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\nplt.title(\"K-Means Clustering\")\nplt.show()\n\n\n\n\n\n\n\nHierarchical Clustering\nHierarchical clustering creates a tree of clusters. It doesn’t require us to pre-specify the number of clusters.\n\nPython Example: Hierarchical Clustering\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom matplotlib import pyplot as plt\n\n# Generating synthetic data\nX, _ = make_blobs(n_samples=50, centers=3, cluster_std=0.70, random_state=0)\n\n# Performing hierarchical clustering\nlinked = linkage(X, 'single')\n\n# Plotting the dendrogram\nplt.figure(figsize=(10, 7))\ndendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\nplt.title(\"Hierarchical Clustering Dendrogram\")\nplt.show()\n\n\n\n\n\n\n\nDBSCAN\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is effective in identifying clusters of arbitrary shapes and handling noise.\n\nPython Example: DBSCAN\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\n\n# Generating synthetic data (two moons)\nX, _ = make_moons(n_samples=300, noise=0.05, random_state=0)\n\n# Applying DBSCAN\ndbscan = DBSCAN(eps=0.3, min_samples=5)\nclusters = dbscan.fit_predict(X)\n\n# Plotting the clusters\nplt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', s=50)\nplt.title(\"DBSCAN Clustering\")\nplt.show()"
  },
  {
    "objectID": "posts/Clustering/Clustering.html#quality-matters",
    "href": "posts/Clustering/Clustering.html#quality-matters",
    "title": "Clustering: Making Sense of Seemingly Random Data",
    "section": "Quality Matters",
    "text": "Quality Matters\n\nSilhouette Coefficient\nThe Silhouette Coefficient is a measure of how similar an object is to its own cluster compared to other clusters.\n\nPython Example: Silhouette Coefficient\n\nfrom sklearn.metrics import silhouette_score\n\n# Assume X and y_kmeans from the K-Means example\nsilhouette_avg = silhouette_score(X, y_kmeans)\nprint(f\"Silhouette Coefficient: {silhouette_avg:.2f}\")\n\nSilhouette Coefficient: -0.04\n\n\nWe calculate the Silhouette Coefficient for the K-means clustering of our synthetic data. A higher coefficient suggests better-defined clusters."
  },
  {
    "objectID": "posts/Clustering/Clustering.html#implementation",
    "href": "posts/Clustering/Clustering.html#implementation",
    "title": "Clustering: Making Sense of Seemingly Random Data",
    "section": "Implementation",
    "text": "Implementation\n\nIris Dataset Clustering\nThe Iris dataset is a classic dataset used in many clustering examples. Here, we apply K-Means to cluster iris flowers based on sepal length and width.\n\nPython Example: Iris Dataset with K-Means\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Loading the Iris dataset\niris = load_iris()\nX = iris.data\n\n# Applying K-Means\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\ny_kmeans = kmeans.predict(X)\n\n# Plotting the clusters\nplt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\nplt.xlabel(\"Sepal Length\")\nplt.ylabel(\"Sepal Width\")\nplt.title(\"K-Means Clustering on Iris Dataset\")\nplt.show()\n\n\n\n\nThis example shows K-Means clustering on the Iris dataset. The resulting plot helps visualize how the algorithm has grouped the flowers into clusters."
  },
  {
    "objectID": "posts/Clustering/Clustering.html#conclusion",
    "href": "posts/Clustering/Clustering.html#conclusion",
    "title": "Clustering: Making Sense of Seemingly Random Data",
    "section": "Conclusion",
    "text": "Conclusion\nClustering is a powerful tool in data analysis, helping us uncover hidden patterns and understand the intrinsic structure of data. While we covered some fundamental clustering techniques, the field is vast, with many more advanced methods and applications. As data continues to grow in size and complexity, the importance and capabilities of clustering algorithms are only set to increase."
  },
  {
    "objectID": "posts/Regression/Regression.html#introduction",
    "href": "posts/Regression/Regression.html#introduction",
    "title": "Regression: Unraveling the Intricacies",
    "section": "Introduction",
    "text": "Introduction\nRegression analysis is a cornerstone of statistics and machine learning, offering insights into the relationships between variables. This post explores two fundamental types: Linear and Non-Linear Regression."
  },
  {
    "objectID": "posts/Regression/Regression.html#linear-regression",
    "href": "posts/Regression/Regression.html#linear-regression",
    "title": "Regression: Unraveling the Intricacies",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nWhat is Linear Regression?\nLinear regression is a statistical method for modeling the relationship between a dependent variable and one or more independent variables using a linear equation.\n\n\nThe Equation\nThe equation for a simple linear regression (one independent variable) is:\n\\[ y = \\beta_0 + \\beta_1x + \\epsilon\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generating synthetic data for linear regression\nnp.random.seed(0)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# Implementing Linear Regression from scratch\nclass LinearRegression:\n    def __init__(self):\n        self.weights = None\n\n    def fit(self, X, y):\n        # Adding a column of ones for the intercept\n        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n        self.weights = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n\n    def predict(self, X):\n        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n        return X_b.dot(self.weights)\n\n# Creating and training the model\nmodel = LinearRegression()\nmodel.fit(X, y)\npredictions = model.predict(X)\n\n# Plotting the data and the regression line\nplt.scatter(X, y, color='blue', label='Data points')\nplt.plot(X, predictions, color='red', label='Regression line')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear Regression')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/Regression/Regression.html#non-linear-regression",
    "href": "posts/Regression/Regression.html#non-linear-regression",
    "title": "Regression: Unraveling the Intricacies",
    "section": "Non-Linear Regression",
    "text": "Non-Linear Regression\n\nWhat is Non-Linear Regression?\nNon-Linear Regression is used for more complex data where a linear model is insufficient. It captures the non-linear relationships between the dependent and independent variables.\n\n\nThe Equation\nA common form of non-linear regression is polynomial regression. The equation for a second-degree polynomial regression is:\n\\[y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\epsilon\\] This equation can be extended to higher degrees depending on the data complexity.\n\n# Re-running the corrected code for non-linear (polynomial) regression\nimport numpy as np\nimport operator\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Generating synthetic data for non-linear (polynomial) regression\nnp.random.seed(0)\nX_nl = 2 - 3 * np.random.normal(0, 1, 100)\ny_nl = X_nl - 2 * (X_nl ** 2) + np.random.normal(-3, 3, 100)\n\n# Transforming the data to include polynomial features\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(X_nl.reshape(-1, 1))\n\n# Reusing the LinearRegression class for polynomial regression\nmodel_nl = LinearRegression()\nmodel_nl.fit(X_poly, y_nl.reshape(-1, 1))\npredictions_nl = model_nl.predict(X_poly)\n\n# Sorting values for a smooth line plot\nsort_axis = operator.itemgetter(0)\nsorted_zip = sorted(zip(X_nl, predictions_nl), key=sort_axis)\nX_nl_sorted, predictions_nl_sorted = zip(*sorted_zip)\n\n# Plotting the data and the polynomial regression curve\nplt.scatter(X_nl, y_nl, color='green', label='Data points')\nplt.plot(X_nl_sorted, predictions_nl_sorted, color='magenta', label='Polynomial regression curve')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Non-Linear (Polynomial)')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/Regression/Regression.html#the-difference",
    "href": "posts/Regression/Regression.html#the-difference",
    "title": "Regression: Unraveling the Intricacies",
    "section": "The Difference",
    "text": "The Difference\nLinear regression’s simplicity is both its strength and limitation. Non-linear regression, while more complex, can model complex relationships in data. The choice depends on the data and the problem at hand."
  },
  {
    "objectID": "posts/Regression/Regression.html#conclusion",
    "href": "posts/Regression/Regression.html#conclusion",
    "title": "Regression: Unraveling the Intricacies",
    "section": "Conclusion",
    "text": "Conclusion\nUnderstanding the theoretical underpinnings of linear and non-linear regression, along with their practical applications, is essential for effective data analysis. By grasping these concepts and mastering their implementations, you can unlock deeper insights from your data."
  }
]